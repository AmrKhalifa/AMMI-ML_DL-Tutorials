{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMMI NLP - Part 1\n",
    "## Lab 1: Introduction to text classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Text Classification with Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you'll implement naive Bayes classifier to classify the text. \n",
    "you need to build a model that predicts the langauge of the text given the words of the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys, math, re\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (string): path to file to be read\n",
    "    \n",
    "    Return: \n",
    "    List of tuples (explained in first question)\n",
    "    '''\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        data.append((tokens[0], tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__deu',\n",
       " ['Ich', 'würde', 'alles', 'tun,', 'um', 'dich', 'zu', 'beschützen.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(\"train1.txt\")\n",
    "data[0]\n",
    "# Tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "    \n",
    "    data is  list of [(label, words), (label, worlds), ......]\n",
    "    list of tuples in the shape (string, [list of strings]) )\n",
    "    \n",
    "    Returns: \n",
    "    \n",
    "    This function should return a dictionary containing the following:\n",
    "    { \n",
    "    # label_counts (python dictionary): \n",
    "         {label:  no. of times the label appeared },\n",
    "    # word_counts  (dictionary of dictionaries): \n",
    "         {label: {word: no. of times this word appeared with this label }},\n",
    "    # label_total (int): \n",
    "        total number of labels. (size of train data),\n",
    "    # word_total  (python dictionary) total number of words (from the entire corupus) of the particular label:\n",
    "          {label: no.of words}\n",
    "          \n",
    "          }\n",
    "    \n",
    "    '''\n",
    "    label_total = 0\n",
    "    word_total = defaultdict(lambda: 0)\n",
    "    label_counts = defaultdict(lambda: 0)\n",
    "    word_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    for example in data:\n",
    "        label, sentence = example\n",
    "        ## FILL CODE\n",
    "\n",
    "\n",
    "    return {'label_counts': label_counts, \n",
    "            'word_counts': word_counts, \n",
    "            'label_total': label_total, \n",
    "            'word_total': word_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, mu, label_counts, word_counts, label_total, word_total):\n",
    "    '''\n",
    "     Parameters: \n",
    "        sentence (string): sentence to be classified\n",
    "        mu (positive real number): Laplace Smoothing hyperparameter\n",
    "        ** The other parameters introduced in the count_words function\n",
    "    \n",
    "    Returns:\n",
    "    best_label (string): the label that has the highest score. \n",
    "    \n",
    "    Implement the function to predict the best label for the given sentence using Naive Bayes algorithm \n",
    "    \n",
    "    '''\n",
    "    best_label = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for label in word_counts.keys():\n",
    "        score = 0.0\n",
    "        ## FILL CODE\n",
    "\n",
    "    return best_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(valid_data, mu, counts):\n",
    "    '''\n",
    "    Parameters:\n",
    "    valid_data (list of tuples): returned value of load_data function \n",
    "    mu (positive real): Laplace smoothing hyper-parameter\n",
    "    counts (dictionary of dictionaries): return value of count_words_function\n",
    "    \n",
    "    Returns: \n",
    "    accuracy (float): the accuracy of the Naive Bayes classifier\n",
    "    '''\n",
    "    accuracy = 0.0\n",
    "    for label, sentence in valid_data:\n",
    "         ## FILL CODE\n",
    "            pass \n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Naive Bayes **\n",
      "\n",
      "Validation accuracy: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Naive Bayes **\")\n",
    "print(\"\")\n",
    "\n",
    "mu = 1.0\n",
    "train_data = load_data(\"train1.txt\")\n",
    "valid_data = load_data(\"valid1.txt\")\n",
    "counts = count_words(train_data)\n",
    "\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(valid_data, mu, counts))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Softmax Classification of Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(filename, threshold=1):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    word_dict, label_dict = {}, {}\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        if not label in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "\n",
    "        for w in tokens[1:]:\n",
    "            counts[w] += 1\n",
    "            \n",
    "    for k, v in counts.items():\n",
    "        if v > threshold:\n",
    "            word_dict[k] = len(word_dict)\n",
    "    return word_dict, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, label_dict = build_dict(\"train1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, word_dict, label_dict):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    dim = len(word_dict)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        yi = label_dict[label]\n",
    "        xi = np.zeros(dim)\n",
    "        for word in tokens[1:]:\n",
    "            if word in word_dict:\n",
    "                wid = word_dict[word]\n",
    "                xi[wid] += 1.0\n",
    "        data.append((yi, xi))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_data(\"train1.txt\", word_dict, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ## FILL CODE\n",
    "    m = x.max()\n",
    "    y = np.exp(x - m)\n",
    "    return y / np.sum(y)\n",
    "    #return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, data, niter):\n",
    "    nlabels, dim = w.shape\n",
    "    for iter in range(niter):\n",
    "        ## FILL CODE\n",
    "        train_loss = 0.0\n",
    "        for yi, xi in train_data:\n",
    "            # We compute the prediction of model and loss\n",
    "            prediction = softmax(np.dot(w, xi))\n",
    "            train_loss += -math.log(prediction[yi])\n",
    "            # We compute the gradient w.r.t. to w\n",
    "            target = np.zeros(nlabels)\n",
    "            target[yi] = 1.0\n",
    "            error = prediction - target\n",
    "            gradient = error.reshape((nlabels, 1)) * xi.reshape((1, dim))\n",
    "            # We apply the gradient step\n",
    "            w = w - 0.5 * gradient\n",
    "        print(\"Iter: %02d    Loss: %.4f\" % (iter, train_loss / len(data)))\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    ## FILL CODE\n",
    "    return np.argmax(softmax(np.dot(w, x)))\n",
    "    #return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(w, valid_data):\n",
    "    ## FILL CODE\n",
    "    accuracy = 0.0\n",
    "    for yi, xi in valid_data:\n",
    "        yp = predict(w, xi)\n",
    "        if yp == yi:\n",
    "            accuracy += 1.0\n",
    "    return accuracy / len(valid_data)\n",
    "    #return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Logistic Regression **\n",
      "\n",
      "Iter: 00    Loss: 0.4906\n",
      "Iter: 01    Loss: 0.2052\n",
      "Iter: 02    Loss: 0.1557\n",
      "Iter: 03    Loss: 0.1313\n",
      "Iter: 04    Loss: 0.1167\n",
      "\n",
      "Validation accuracy: 0.933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Logistic Regression **\")\n",
    "print(\"\")\n",
    "\n",
    "word_dict, label_dict = build_dict(\"train1.txt\")\n",
    "train_data = load_data(\"train1.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"valid1.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "w = sgd(w, train_data, 5)\n",
    "print(\"\")\n",
    "print(\"Validation accuracy: %.3f\" % compute_accuracy(w, valid_data))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
